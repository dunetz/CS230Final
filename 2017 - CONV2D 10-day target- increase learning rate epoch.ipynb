{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yp5xcGQP0iow"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "j4ddX9Wp0m6h",
    "outputId": "04ab0ff6-7dbf-4fad-e41e-0655a078fa22"
   },
   "outputs": [],
   "source": [
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U4OI2JUT01pY"
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#os.chdir(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iEyds35W07YL",
    "outputId": "7f52a6ce-09b9-4b27-9f0e-734b1f140579"
   },
   "outputs": [],
   "source": [
    "#cd My Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "z3z6DN5o1C1U",
    "outputId": "0ec8c793-312f-46a0-c191-c88c0b589a9b"
   },
   "outputs": [],
   "source": [
    "#cd CS230-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y0PpAUvr0Omk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import time\n",
    "%matplotlib inline\n",
    "# \n",
    "from sklearn.metrics import confusion_matrix, classification_report, cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_-lC-kwq0Omo",
    "outputId": "3257b983-4c41-48f0-b0c4-fdca7da177ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os; \n",
    "#work-around to allow import from directory on same level\n",
    "os.sys.path.append(os.path.dirname(os.path.abspath('.')))\n",
    "from input.DataLoader import DataLoader\n",
    "from input.GenerateBatch import GenerateBatch\n",
    "from model.Model3 import train_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDkNW07Q0Omu"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense,Dropout,Activation, Lambda\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling1D, Flatten\n",
    "from keras.layers import  MaxPooling1D, MaxPooling2D,Reshape, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from scipy.stats import linregress\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, cohen_kappa_score\n",
    "from keras.backend import transpose, permute_dimensions,squeeze\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "SnCCfwPL0Omv",
    "outputId": "8fba1a7f-a9ff-4e6f-e91e-c72c0162e86d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainday [0, 1, 2, 3, 4, 5] valdays [6, 7] colx range(0, 40) coly 148\n",
      "{'batch_size': 50, 'sequence_length': 100, 'epochs': 100, 'learning_rate': 0.001, 'learning_rate_decay': 0, 'opt': <keras.optimizers.Adam object at 0x7f70d3e40f98>, 'save_dir': 'saved_models', 'nflag': False}\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS AND COMMENTs\n",
    "#\n",
    "# Conv2D with 10bar target and 0-5.6-7 training/val \n",
    "# standardize window = false\n",
    "# Dropout back to 50; epochs back to 60\n",
    "# Try l2 reg on dense layer with small batch size - 50\n",
    "configs={}\n",
    "configs['batch_size']=50\n",
    "\n",
    "configs['sequence_length']=100\n",
    "configs['epochs']=100\n",
    "configs['learning_rate']=.001\n",
    "configs['learning_rate_decay']=0\n",
    "configs['opt']=Adam(lr=configs['learning_rate'],\n",
    "                   decay=configs['learning_rate_decay'])\n",
    "configs['save_dir']='saved_models'\n",
    "configs['nflag']=False #standardize each window\n",
    "\n",
    "\n",
    "\n",
    "traindays=[0,1,2,3,4,5] #training and val days to look at in dataset\n",
    "valdays=[6,7]\n",
    "colx=range(40)# columns to look at in dataset - 0-40 are LOB\n",
    "#colx=np.concatenate([np.arange(20),86+np.arange(20)])\n",
    "coly=148 # target 144-148 are 1,2,3,5,10\n",
    "generate_files=False\n",
    "print('trainday',traindays,'valdays',valdays,'colx',colx,'coly',coly)\n",
    "print(configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mNamKqckz6f7"
   },
   "source": [
    "### Generate data from zip files or retrieve from pkl files in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IGGsKI3ez6gA"
   },
   "outputs": [],
   "source": [
    "if generate_files==True:\n",
    "  path='data/'\n",
    "  d=DataLoader(path)\n",
    "  traindf=d.get_days(traindays)\n",
    "  trainoffset=d.get_stock_offset(traindays)\n",
    "  valdf=d.get_days(valdays)\n",
    "  valoffset=d.get_stock_offset(valdays)\n",
    "\n",
    "  traindf.to_pickle(\"traindf.pkl\")\n",
    "  valdf.to_pickle(\"valdf.pkl\")\n",
    "  np.save(\"trainoffset\",trainoffset)\n",
    "  np.save(\"valoffset\",valoffset)\n",
    "\n",
    "else:\n",
    "  traindf=pd.read_pickle('traindf.pkl')\n",
    "  valdf=pd.read_pickle('valdf.pkl')\n",
    "  trainoffset=np.load(\"trainoffset.npy\")\n",
    "  valoffset=np.load(\"valoffset.npy\")\n",
    "\n",
    "# Select X and Y training and dev data. Y is categorical\n",
    "trainx=traindf.iloc[:,colx].values\n",
    "trainy=to_categorical(traindf.iloc[:,coly].values-1)\n",
    "valx=valdf.iloc[:,colx].values\n",
    "valy=to_categorical(valdf.iloc[:,coly].values-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rR0QQ6eBz6gC"
   },
   "source": [
    "### Build and compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "uZY-jfT70Om6",
    "outputId": "4ecc4ab7-ea7a-4975-c5ec-3b731ace975b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 97, 1, 16)         2576      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 97, 1, 16)         272       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 94, 1, 32)         2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 47, 1, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 45, 1, 32)         3104      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 22, 1, 32)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 704)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               70500     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 79,235\n",
      "Trainable params: 79,035\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(16, kernel_size=(4, 40), strides=(1, 1),data_format='channels_last',activation='relu',\n",
    "                 input_shape=(configs['sequence_length'],len(colx),1)))\n",
    "model.add(Conv2D(16, kernel_size=(1,1),strides=(1,1),data_format='channels_last',activation='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2,1), strides=2))\n",
    "model.add(Conv2D(32,kernel_size=(4,1), strides=(1,1),data_format='channels_last',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,1), strides=2))\n",
    "model.add(Conv2D(32,kernel_size=(3,1), strides=(1,1),data_format='channels_last',activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,1), strides=2))\n",
    "model.add(Flatten())\n",
    "#model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(100,kernel_regularizer=regularizers.l2(0.01),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.50))\n",
    "#model.add(Dense(64,kernel_regularizer=regularizers.l2(0.01),activation='relu'))\n",
    "#model.add(Dropout(0.20))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "#\n",
    "opt=Adam(lr=configs['learning_rate'],decay=configs['learning_rate_decay'])\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=configs['opt'],metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VtnITiYbz6gF"
   },
   "source": [
    "### Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pyyecGn40Om-"
   },
   "outputs": [],
   "source": [
    "training_generator=GenerateBatch(trainx,trainy,trainoffset)\n",
    "val_generator=GenerateBatch(valx,valy,valoffset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w0b2kB8y0OnA"
   },
   "outputs": [],
   "source": [
    "# out-of memory generative training\n",
    "steps_per_epoch=training_generator.epoch_size(configs['sequence_length'],\n",
    "                                              configs['batch_size'])\n",
    "steps_per_epoch_val=training_generator.epoch_size(configs['sequence_length'],\n",
    "                                              configs['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2125
    },
    "colab_type": "code",
    "id": "blrriHpG0OnC",
    "outputId": "8f379ed0-eb89-4f0c-e4e7-021cf7811be1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Model] Training Started\n",
      "[Model] 100 epochs, 50 batch size, 4305.0 batches per epoch\n",
      "Epoch 1/100\n",
      " - 41s - loss: 1.0788 - acc: 0.4484 - val_loss: 8.6407 - val_acc: 0.3567\n",
      "Epoch 2/100\n",
      " - 37s - loss: 1.0113 - acc: 0.4982 - val_loss: 10.2309 - val_acc: 0.3657\n",
      "Epoch 3/100\n",
      " - 38s - loss: 1.0243 - acc: 0.4607 - val_loss: 1.8014 - val_acc: 0.3485\n",
      "Epoch 4/100\n",
      " - 38s - loss: 1.0286 - acc: 0.4623 - val_loss: 2.1207 - val_acc: 0.3673\n",
      "Epoch 5/100\n",
      " - 38s - loss: 1.0252 - acc: 0.4679 - val_loss: 10.2456 - val_acc: 0.3646\n",
      "Epoch 6/100\n",
      " - 38s - loss: 1.0354 - acc: 0.4490 - val_loss: 2.1657 - val_acc: 0.3516\n",
      "Epoch 7/100\n",
      " - 38s - loss: 1.0295 - acc: 0.4587 - val_loss: 3.1225 - val_acc: 0.3665\n",
      "Epoch 8/100\n",
      " - 38s - loss: 1.0165 - acc: 0.4919 - val_loss: 2.0903 - val_acc: 0.4067\n",
      "Epoch 9/100\n",
      " - 38s - loss: 1.0251 - acc: 0.4624 - val_loss: 3.1797 - val_acc: 0.3525\n",
      "Epoch 10/100\n",
      " - 38s - loss: 1.0219 - acc: 0.4663 - val_loss: 2.7473 - val_acc: 0.3664\n",
      "Epoch 11/100\n",
      " - 38s - loss: 1.0198 - acc: 0.4681 - val_loss: 2.2810 - val_acc: 0.3577\n",
      "Epoch 12/100\n",
      " - 39s - loss: 1.0187 - acc: 0.4703 - val_loss: 1.2815 - val_acc: 0.4116\n",
      "Epoch 13/100\n",
      " - 38s - loss: 1.0160 - acc: 0.4732 - val_loss: 1.5047 - val_acc: 0.3946\n",
      "Epoch 14/100\n",
      " - 38s - loss: 1.0157 - acc: 0.4733 - val_loss: 1.1618 - val_acc: 0.4277\n",
      "Epoch 15/100\n",
      " - 38s - loss: 1.0133 - acc: 0.4773 - val_loss: 1.6724 - val_acc: 0.3748\n",
      "Epoch 16/100\n",
      " - 38s - loss: 1.0081 - acc: 0.4793 - val_loss: 2.8795 - val_acc: 0.3810\n",
      "Epoch 17/100\n",
      " - 38s - loss: 1.0050 - acc: 0.4812 - val_loss: 2.2769 - val_acc: 0.3936\n",
      "Epoch 18/100\n",
      " - 38s - loss: 1.0026 - acc: 0.4841 - val_loss: 1.4049 - val_acc: 0.4228\n",
      "Epoch 19/100\n",
      " - 38s - loss: 1.0007 - acc: 0.4846 - val_loss: 1.6046 - val_acc: 0.4206\n",
      "Epoch 20/100\n",
      " - 38s - loss: 0.9990 - acc: 0.4864 - val_loss: 1.9603 - val_acc: 0.4076\n",
      "Epoch 21/100\n",
      " - 38s - loss: 0.9959 - acc: 0.4895 - val_loss: 1.4308 - val_acc: 0.4345\n",
      "Epoch 22/100\n",
      " - 38s - loss: 0.9929 - acc: 0.4895 - val_loss: 1.6130 - val_acc: 0.4347\n",
      "Epoch 23/100\n",
      " - 38s - loss: 0.9910 - acc: 0.4919 - val_loss: 2.8218 - val_acc: 0.3768\n",
      "Epoch 24/100\n",
      " - 38s - loss: 0.9904 - acc: 0.4925 - val_loss: 1.4452 - val_acc: 0.4319\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-01522590cf95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'save_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CS230-Master/model/Model3.py\u001b[0m in \u001b[0;36mtrain_generator\u001b[0;34m(model, train_gen, val_gen, epochs, batch_size, steps_per_epoch, validation_steps, save_dir)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         )\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    232\u001b[0m                             \u001b[0mval_enqueuer_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                             \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                             workers=0)\n\u001b[0m\u001b[1;32m    235\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                         \u001b[0;31m# No need for try/except because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m   1470\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m             verbose=verbose)\n\u001b[0m\u001b[1;32m   1473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[0;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[1;32m    344\u001b[0m                                  \u001b[0;34m'or (x, y). Found: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                                  str(generator_output))\n\u001b[0;32m--> 346\u001b[0;31m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mouts_per_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[0;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[1;32m   1254\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# GenerateBatch2 contains logic to avoid running over stock/day divides\n",
    "history=train_generator(\n",
    "    model=model,\n",
    "    train_gen=training_generator.GenerateBatch_conv(configs['sequence_length'],\n",
    "                                               configs['batch_size'],\n",
    "                                               configs['nflag']),\n",
    "    val_gen=val_generator.GenerateBatch_conv(configs['sequence_length'],\n",
    "                                        configs['batch_size'],\n",
    "                                        configs['nflag']),\n",
    "    epochs=configs['epochs'],\n",
    "    batch_size=configs['batch_size'],\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=steps_per_epoch_val,\n",
    "    save_dir=configs['save_dir']\n",
    "    )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sM5Mq6Eiz6gP"
   },
   "outputs": [],
   "source": [
    "#from keras.models import load_model\n",
    "#model = load_model('saved_models/04122018-125307-e20.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "colab_type": "code",
    "id": "DPDi5kAW0OnF",
    "outputId": "d535f7a3-4c92-499b-f77f-1281f5c99fe9"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(history.history['loss'],'b--',lw=2,label='train_loss')\n",
    "plt.plot(history.history['val_loss'],'g-',lw=2,label='val_loss')\n",
    "plt.legend()\n",
    "plt.ylim([.5,1.3])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5CscUax0OnL"
   },
   "outputs": [],
   "source": [
    "# make our predictions - start from beginning\n",
    "train_gen=training_generator.GenerateBatch_conv(configs['sequence_length'],configs['batch_size'],configs['nflag'])\n",
    "val_gen=val_generator.GenerateBatch_conv(configs['sequence_length'],configs['batch_size'],configs['nflag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dSJU9k7C0OnO"
   },
   "outputs": [],
   "source": [
    "y_train=[]\n",
    "yhat_train=[]\n",
    "y_val=[]\n",
    "yhat_val=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mv-Bygy70OnR"
   },
   "outputs": [],
   "source": [
    "# predict using same sequence lengths and batches as in training\n",
    "for i in range(int(steps_per_epoch)):\n",
    "    dat=next(train_gen)\n",
    "    y_train.append(dat[1])\n",
    "    yhat_train.append(model.predict(dat[0]))\n",
    "for i in range(int(steps_per_epoch_val)):\n",
    "    dat=next(val_gen)\n",
    "    y_val.append(dat[1])\n",
    "    yhat_val.append(model.predict(dat[0]))\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jBtQSLKI0OnY"
   },
   "outputs": [],
   "source": [
    "burn_in=0 # drop early predictions \n",
    "y_train=np.concatenate(y_train).reshape(-1,3)[burn_in:]\n",
    "yhat_train=np.concatenate(yhat_train).reshape(-1,3)[burn_in:]\n",
    "y_val=np.concatenate(y_val).reshape(-1,3)[burn_in:]\n",
    "yhat_val=np.concatenate(yhat_val).reshape(-1,3)[burn_in:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OcFMwGttz6gg"
   },
   "source": [
    "### Training and Validation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "ESxiLCRb0One",
    "outputId": "b0d613e2-6821-4e73-f1f5-3b3131b40423"
   },
   "outputs": [],
   "source": [
    "# How well have we done on training data\n",
    "print (\"Training \")\n",
    "Y=np.argmax(y_train,axis=1)\n",
    "Yhat=np.argmax(yhat_train,axis=1)\n",
    "c=confusion_matrix(Y,Yhat)\n",
    "c=np.concatenate((c,np.sum(c,axis=1).reshape(-1,1)),axis=1)\n",
    "c=np.concatenate((c,np.sum(c,axis=0).reshape(1,-1)),axis=0)\n",
    "#print(c)\n",
    "print(classification_report(Yhat,Y))\n",
    "print(\"cohen kappa score: %.2f\" %cohen_kappa_score(Yhat,Y))\n",
    "# How well have we done on test data\n",
    "print(\"Test \")\n",
    "Y=np.argmax(y_val,axis=1)\n",
    "Yhat=np.argmax(yhat_val,axis=1)\n",
    "c=confusion_matrix(Y,Yhat)\n",
    "c=np.concatenate((c,np.sum(c,axis=1).reshape(-1,1)),axis=1)\n",
    "c=np.concatenate((c,np.sum(c,axis=0).reshape(1,-1)),axis=0)\n",
    "#print(c)\n",
    "print(classification_report(Yhat,Y))\n",
    "print(\"cohen kappa score: %.2f\" % cohen_kappa_score(Yhat,Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hsWpBeiXz6gi"
   },
   "source": [
    "### Visualize Filters using Gradient Ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_ydeUWJz6gk"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8iUJv69nz6gl"
   },
   "outputs": [],
   "source": [
    "layer_dict=dict([(layer.name,layer) for layer in model.layers[1::]])\n",
    "input_img=model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7C6eQbiIz6gn"
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "     return x/(K.sqrt(K.mean(K.square(x)))+K.epsilon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ah9UhDrgz6gq"
   },
   "outputs": [],
   "source": [
    "layer_name='conv2d_4'\n",
    "n=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "Ww5BEPHdz6gs",
    "outputId": "aebe7f3e-ad39-49f1-b002-728643407094"
   },
   "outputs": [],
   "source": [
    "kept_filters=[]\n",
    "for filter_index in range(n):\n",
    "    layer_output=layer_dict[layer_name].output\n",
    "    loss=K.mean(layer_output[:,filter_index:,:])\n",
    "    grads=K.gradients(loss,input_img)[0]\n",
    "    grads=normalize(grads)\n",
    "    iterate=K.function([input_img],[loss,grads])\n",
    "    step=1\n",
    "    input_img_data=np.random.random((1,100,40,1))\n",
    "    for i in range(40):\n",
    "        loss_value,grad_values=iterate([input_img_data])\n",
    "        input_img_data +=grad_values*step\n",
    "    print('Filter: %d, Final loss value:%.2f' % (filter_index,loss_value))\n",
    "    if loss_value >0:\n",
    "        kept_filters.append((input_img_data[0],loss_value))\n",
    "print(len(kept_filters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Velql3gqz6gu",
    "outputId": "56051eeb-c65f-4163-a091-7bb9fc7407a5"
   },
   "outputs": [],
   "source": [
    "len(kept_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DoAQKWrez6gx"
   },
   "outputs": [],
   "source": [
    "f0=kept_filters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "To1De-ezz6g0",
    "outputId": "23194718-b745-441f-b44f-8ef69c02ed15"
   },
   "outputs": [],
   "source": [
    "f0[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7qYqr6tOz6g3",
    "outputId": "94716a41-cd13-4f49-deb7-c8b12f227896"
   },
   "outputs": [],
   "source": [
    "f0[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "44L_Lgwyz6g5"
   },
   "outputs": [],
   "source": [
    "img=f0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "97xwHkZzz6g9",
    "outputId": "c962a6f5-fd22-4ae7-cac5-e50a167923b5"
   },
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1074
    },
    "colab_type": "code",
    "id": "mqb0OHa4z6g-",
    "outputId": "c20060ae-da31-498e-bd68-65104ec20e6d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.suptitle(\"How filters visualize the first three price levels\",fontsize=12)\n",
    "for i,f in enumerate(kept_filters[:16]):\n",
    "    img=f[0][:,:,0]\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.plot(img[range(0,100)][:,[0,2,4,6,8,10]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1076
    },
    "colab_type": "code",
    "id": "4zBxVxMqz6hA",
    "outputId": "3500901b-d9c6-4cf3-89a7-2889dcd33d26"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.suptitle(\"How filters visualize the first three price levels\",fontsize=12)\n",
    "for i,f in enumerate(kept_filters[:16]):\n",
    "    img=f[0][:,:,0]\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.plot(img[range(0,100)][:,[1,3,5,7,9,11,13]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "colab_type": "code",
    "id": "TKjObJMQz6hB",
    "outputId": "bc8c1487-6b0f-4b9f-e1a4-c1be7b0dcb97"
   },
   "outputs": [],
   "source": [
    "# visualize one layer across all 16 filters\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.title(\"Inside Ask Across 16 filters\")\n",
    "for f in kept_filters[:16]:\n",
    "    img=f[0][:,:,0]\n",
    "    plt.plot(img[range(0,100)][:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NA2Rc6X5z6hE"
   },
   "source": [
    "###  Find highest predictions for each category in each batch. Plot highest values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygkCC6pJz6hE"
   },
   "outputs": [],
   "source": [
    "dn, sm, up = [],[],[]\n",
    "\n",
    "# find the highest scoring input for each categorical output category in each batch\n",
    "# make our predictions - start from beginning\n",
    "train_gen=training_generator.GenerateBatch_conv(configs['sequence_length'],configs['batch_size'],configs['nflag'])\n",
    "for i in range(int(steps_per_epoch)):\n",
    "    dat=next(train_gen)[0]\n",
    "    yhat=model.predict(dat)\n",
    "    #\n",
    "    dn.append(dat[np.argmax(yhat[:,0])])\n",
    "    sm.append(dat[np.argmax(yhat[:,1])])\n",
    "    up.append(dat[np.argmax(yhat[:,2])])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0tY7cyhHcjbN"
   },
   "outputs": [],
   "source": [
    "dnavg=np.average(np.array(dn),axis=0)\n",
    "upavg=np.average(np.array(up),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "FTkaLRa2kIHh",
    "outputId": "f2399e12-1204-4542-fb65-212e0f56f754"
   },
   "outputs": [],
   "source": [
    "t=np.array([0])\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.suptitle(\"Up predictions\")\n",
    "plt.subplot(1,4,1)\n",
    "_=plt.plot(upavg[:,t,0])\n",
    "plt.title('price:ask')\n",
    "plt.subplot(1,4,2)\n",
    "_=plt.plot(upavg[:,t+2,0])\n",
    "plt.title('price:bid')\n",
    "plt.subplot(1,4,3)\n",
    "_=plt.plot(upavg[:, t+1,0])\n",
    "plt.title('vol:ask')\n",
    "plt.subplot(1,4,4)\n",
    "_=plt.plot(upavg[:,t+3,0])\n",
    "_=plt.title('vol:bid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "id": "bggq3NJTz6hJ",
    "outputId": "7e1775b3-5c8a-4d86-c5ba-50db6b90f379"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.suptitle(\"Down prediction\")\n",
    "plt.subplot(1,4,1)\n",
    "_=plt.plot(dnavg[:,t,0])\n",
    "plt.title('price:ask')\n",
    "plt.subplot(1,4,2)\n",
    "_=plt.plot(dnavg[:,t+2,0])\n",
    "plt.title('price:bid')\n",
    "plt.subplot(1,4,3)\n",
    "_=plt.plot(dnavg[:, t+1,0])\n",
    "plt.title('vol;ask')\n",
    "plt.subplot(1,4,4)\n",
    "_=plt.plot(dnavg[:,t+3,0])\n",
    "_=plt.title('vol:bid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hxtxADQVz6hQ"
   },
   "source": [
    "### Perform Gradient Ascent on each category (final softmax layer) to generate input with strongest signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "ZdS0pNjHz6hQ",
    "outputId": "a1e7519e-e2f6-4774-a020-a3021a0fe5d6"
   },
   "outputs": [],
   "source": [
    "#visualize\n",
    "ylabel=[]\n",
    "layer_name='dense_2' #final layer - categorical\n",
    "layer_output=layer_dict[layer_name].output\n",
    "print(layer_output.shape)\n",
    "for i in range(3):\n",
    "    print('processing label %d' % i)\n",
    "    loss=layer_output[:,i] #want to increase value of label i\n",
    "    #loss=K.mean(layer_output[:,filter_index:,:])\n",
    "    grads=K.gradients(loss,input_img)[0]\n",
    "    grads=normalize(grads)\n",
    "    iterate=K.function([input_img],[loss,grads])\n",
    "    step=1\n",
    "    input_img_data=np.random.random((1,100,40,1))\n",
    "    for j in range(1000):\n",
    "        loss_value,grad_values=iterate([input_img_data])\n",
    "        input_img_data +=grad_values*step\n",
    "        #print('Current loss values:', loss_value)\n",
    "    print(loss_value)\n",
    "    ylabel.append((input_img_data[0],loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "colab_type": "code",
    "id": "izIAyVb2z6hY",
    "outputId": "6af7d863-6a08-4735-8862-1d2c6c5a204a"
   },
   "outputs": [],
   "source": [
    "\n",
    "y=ylabel[0][0]\n",
    "\n",
    "t=np.array([0])\n",
    "#\n",
    "y=ylabel[0][0]\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.title('Generated Image for inside bid/ask for Down Prediction')\n",
    "plt.plot(y[:,t,0])\n",
    "plt.plot(y[:,t+2,0])\n",
    "#\n",
    "y=ylabel[2][0]\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(' Generated Image for inside bid/ask for Up Prediction')\n",
    "plt.plot(y[:,t,0])\n",
    "plt.plot(y[:,t+2,0])\n",
    "y=ylabel[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "grmT4M1Nz6ha"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CONV2D - Master.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
